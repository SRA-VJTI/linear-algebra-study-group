# **Dot Product and Duality**
![](Images/Lect9_1.png)


- Dot Product is projecting a Vector onto other vector and Multiplying their Length , if the projection is in other direction the Dot Product will be Negative , and if Perpendicular then the projection will be zero

![](Images/Lect9_2.png)


- Order dosen’t Matter who projects onto whom

![](Images/Lect9_3.png)


- vector Multiplication (1X2 Matrice = 2D Matrice)

![](Images/Lect9_4.png)


![](Images/Lect9_5.png)


- Duality : Natural correspondence (ie linear transformation of vector is some other vector in that space ) (ie 2 computation that look similar )

# **Cross Product**
![](Images/Lect10_1.png)
- take a vector v and move the vector w to the end of it and then repeat the same in other order (ie take w and move start of v to end of w)the diagram obtained is a parallelogram ) — the Determinant
- +ve and -ve is due to the order (j should be on left of I anticlockwise )

![](Images/Lect10_2.png)
![](Images/Lect10_3.png)
- The Cross product is not a number its a resultant vector as per Right hand thumb rule of the magnitude given by the number
![](Images/Lect10_4.png)

# Cross products in the light of linear transformations 

The **Cross product** a × b is defined as a vector c that is perpendicular (orthogonal) to both a and b, with a direction given by the right-hand rule and a magnitude equal to the area of the parallelogram that the vectors span

![General way of defining the cross product](Images/Lect11_1.png 'General way of defining the cross product')

The linear transformation to the number line can be matched to a vector which is called the dual vector of that transformation, such that performing the linear transformation is same as taking the dot product with that vector.

![](Images/Lect11_2.png )

![](Images/Lect11_3.png )

# Cramer's rule, explained geometrically

An orthogonal transformation is a linear transformation which preserves a symmetric inner product. In particular, an orthogonal transformation (technically, an orthonormal transformation) preserves lengths of vectors and angles between vectors.

![Cramer's formula](Images/Lect12_1.png "Cramer's formula")

![](Images/Lect12_2.png )

# Change of Basis

In linear algebra, a basis for a vector space is a linearly independent set spanning the vector space.

![](Images/Lect13_1.png)

Geometrically the matrix represents transformation from other grid to our grid, but numerically it is exactly opposite.

![](Images/Lect13_2.png)

![](Images/Lect11_3.png)
Inverse of Matrix represents the reverse linear transformation. So the Inverse matrix will be the transformation that will transform the vector from our grid to the other grids.

Translation matrices is not same as transforming vectors. The following pictures show the various steps in translating a matrix from one coordinate system to another.

![](Images/Lect13_4.png)
![](Images/Lect13_5.png)
![](Images/Lect13_6.png)
![](Images/Lect13_7.png)

![](Images/Lect14_1.png)

- when a vector is transformed the span of it also changes but sometimes even after transformation the span doesn't changes
- For those vectors which remain on their span even after transformation but stretched are called the eigen vectors of the transformation and eigen values are the factors by which they are stretched or squished

![](Images/Lect14_2.png)

![](Images/Lect14_3.png)

![](Images/Lect14_4.png)

![](Images/Lect14_5.png)

- We have to find the value of the lambda so that the determinant is zero so that the following assumption becomes true

![](Images/Lect14_6.png)

![](Images/Lect14_7.png)

![](Images/Lect14_8.png)

![](Images/Lect14_9.png)

![](Images/Lect14_10.png)

![](Images/Lect14_11.png)

Not all matrices have eigen basis

![](Images/Lect15_1.png)

![](Images/Lect15_2.png)
